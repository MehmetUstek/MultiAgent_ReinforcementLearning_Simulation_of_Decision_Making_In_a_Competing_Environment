{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "willing-burns",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!..\\venv\\Scripts\\activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "surprising-statistics",
   "metadata": {},
   "outputs": [
    {
     "ename": "Error",
     "evalue": "Cannot re-register id: MyCombat-v10",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mError\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-378d1d7a75ab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'MyCombat-v10'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0mentry_point\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ma_gym.envs.combat.combat2:Combat'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m     \u001b[0mkwargs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'n_agents'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mN_AGENT\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'n_food'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'full_observable'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'step_cost'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m0.05\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m     \u001b[1;31m# It has a step cost of -0.2 now\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m )\n",
      "\u001b[1;32mc:\\users\\mehmet üstek\\untitled9\\gym\\gym\\envs\\registration.py\u001b[0m in \u001b[0;36mregister\u001b[1;34m(id, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mregister\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mregistry\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mehmet üstek\\untitled9\\gym\\gym\\envs\\registration.py\u001b[0m in \u001b[0;36mregister\u001b[1;34m(self, id, **kwargs)\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mid\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv_specs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 135\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Cannot re-register id: {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    136\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv_specs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEnvSpec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mError\u001b[0m: Cannot re-register id: MyCombat-v10"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Created on Wednesday Jan  16 2019\n",
    "@author: Seyed Mohammad Asghari\n",
    "@github: https://github.com/s3yyy3d-m\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import argparse\n",
    "import pandas as pd\n",
    "from training.dqn_agent import Agent\n",
    "from training.brain import Brain\n",
    "# from dqn_agent import Agent\n",
    "import glob\n",
    "import gym\n",
    "\n",
    "N_AGENT = 5\n",
    "ARG_LIST = ['learning_rate', 'optimizer', 'memory_capacity', 'batch_size', 'target_frequency', 'maximum_exploration',\n",
    "            'max_timestep', 'first_step_memory', 'replay_steps', 'number_nodes', 'target_type', 'memory',\n",
    "            'prioritization_scale', 'agents_number', 'grid_size']\n",
    "\n",
    "gym.envs.register(\n",
    "    id='MyCombat-v11',\n",
    "    entry_point='ma_gym.envs.combat.combat2:Combat',\n",
    "    kwargs={'n_agents': N_AGENT,'n_food':20, 'full_observable': False, 'step_cost': -0.05}\n",
    "    # It has a step cost of -0.2 now\n",
    ")\n",
    "\n",
    "def get_name_brain(args, idx):\n",
    "\n",
    "    file_name_str = '_'.join([str(args[x]) for x in ARG_LIST])\n",
    "\n",
    "    return './results_agents/weights_files/' + file_name_str + '_' + str(idx) + '.h5'\n",
    "\n",
    "\n",
    "def get_name_rewards(args):\n",
    "\n",
    "    file_name_str = '_'.join([str(args[x]) for x in ARG_LIST])\n",
    "\n",
    "    return './results_agents/rewards_files/' + file_name_str + '.csv'\n",
    "\n",
    "\n",
    "def get_name_timesteps(args):\n",
    "\n",
    "    file_name_str = '_'.join([str(args[x]) for x in ARG_LIST])\n",
    "\n",
    "    return './results_agents/timesteps_files/' + file_name_str + '.csv'\n",
    "\n",
    "\n",
    "class Environment(object):\n",
    "\n",
    "    def __init__(self, arguments):\n",
    "        current_path = os.path.dirname(__file__)  # Where your .py file is located\n",
    "        self.env = gym.make('MyCombat-v11')\n",
    "        self.episodes_number = arguments['episode_number']\n",
    "        self.render = arguments['render']\n",
    "        self.recorder = arguments['recorder']\n",
    "        self.max_ts = arguments['max_timestep']\n",
    "        self.test = arguments['test']\n",
    "        self.filling_steps = arguments['first_step_memory']\n",
    "        self.steps_b_updates = arguments['replay_steps']\n",
    "        self.max_random_moves = arguments['max_random_moves']\n",
    "\n",
    "        self.num_agents = arguments['agents_number']\n",
    "        self.num_landmarks = self.num_agents\n",
    "        self.grid_size = arguments['grid_size']\n",
    "\n",
    "    def run(self, agents, file1, file2):\n",
    "\n",
    "        total_step = 0\n",
    "        rewards_list = []\n",
    "        timesteps_list = []\n",
    "        max_score = -10000\n",
    "        for episode_num in range(self.episodes_number):\n",
    "            state = self.env.reset()\n",
    "            if self.render:\n",
    "                self.env.render()\n",
    "\n",
    "            random_moves = random.randint(0, self.max_random_moves)\n",
    "\n",
    "            # create randomness in initial state\n",
    "            # for _ in range(random_moves):\n",
    "            #     actions = [4 for _ in range(len(agents))]\n",
    "            #     state, _, _ = self.env.step(actions)\n",
    "            #     if self.render:\n",
    "            #         self.env.render()\n",
    "\n",
    "            # converting list of positions to an array\n",
    "            # print(state)\n",
    "            state = np.array(state)\n",
    "            state = state.ravel()\n",
    "\n",
    "            done = [False for _ in range(self.num_agents)]\n",
    "            reward_all = 0\n",
    "            time_step = 0\n",
    "            print(done)\n",
    "            while not all(done):\n",
    "\n",
    "                # if self.render:\n",
    "                #     self.env.render()\n",
    "                actions = []\n",
    "                for agent in agents:\n",
    "                    actions.append(agent.greedy_actor(state))\n",
    "                next_state, reward, done, info = self.env.step(actions)\n",
    "                # converting list of positions to an array\n",
    "                next_state = np.array(next_state)\n",
    "                next_state = next_state.ravel()\n",
    "                # print(\"next state\",next_state)\n",
    "\n",
    "                if not self.test:\n",
    "                    for agent in agents:\n",
    "                        agent.observe((state, actions, reward, next_state, done))\n",
    "                        if total_step >= self.filling_steps:\n",
    "                            agent.decay_epsilon()\n",
    "                            # print(\"b_updates\",self.steps_b_updates)\n",
    "                            # print(\"time_step\",time_step)\n",
    "                            if time_step % self.steps_b_updates == 0:\n",
    "                                print(\"replay happened\")\n",
    "                                agent.replay()\n",
    "                            agent.update_target_model()\n",
    "\n",
    "                total_step += 1\n",
    "                time_step += 1\n",
    "                state = next_state\n",
    "                reward_all += sum(reward)\n",
    "\n",
    "                if self.render:\n",
    "                    self.env.render()\n",
    "\n",
    "            rewards_list.append(reward_all)\n",
    "            timesteps_list.append(time_step)\n",
    "\n",
    "            print(\"Episode {p}, Score: {s}, Final Step: {t}, Goal: {g}\".format(p=episode_num, s=reward_all,\n",
    "                                                                               t=time_step, g=done))\n",
    "\n",
    "            if self.recorder:\n",
    "                os.system(\"ffmpeg -r 2 -i ./results_agents/snaps/%04d.png -b:v 40000 -minrate 40000 -maxrate 4000k -bufsize 1835k -c:v mjpeg -qscale:v 0 \"\n",
    "                          + \"./results_agents/videos/{a1}_{a2}.avi\".format(a1=self.num_agents,a2=self.grid_size))\n",
    "                files = glob.glob('./results_agents/snaps/*')\n",
    "                for f in files:\n",
    "                    os.remove(f)\n",
    "\n",
    "            if not self.test:\n",
    "                if episode_num % 30 == 0:\n",
    "                    df = pd.DataFrame(rewards_list, columns=['score'])\n",
    "                    df.to_csv(file1)\n",
    "\n",
    "                    df = pd.DataFrame(timesteps_list, columns=['steps'])\n",
    "                    df.to_csv(file2)\n",
    "\n",
    "                    if total_step >= self.filling_steps:\n",
    "                        if reward_all > max_score:\n",
    "                            for agent in agents:\n",
    "                                agent.brain.save_model()\n",
    "                            max_score = reward_all\n",
    "\n",
    "\n",
    "if __name__ ==\"__main__\":\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    # DQN Parameters\n",
    "    parser.add_argument('-e', '--episode-number', default=1000000, type=int, help='Number of episodes')\n",
    "    parser.add_argument('-l', '--learning-rate', default=0.00005, type=float, help='Learning rate')\n",
    "    parser.add_argument('-op', '--optimizer', choices=['Adam', 'RMSProp'], default='Adam',\n",
    "                        help='Optimization method')\n",
    "    parser.add_argument('-m', '--memory-capacity', default=1000000, type=int, help='Memory capacity')\n",
    "    parser.add_argument('-b', '--batch-size', default=64, type=int, help='Batch size')\n",
    "    parser.add_argument('-t', '--target-frequency', default=10000, type=int,\n",
    "                        help='Number of steps between the updates of target network')\n",
    "    parser.add_argument('-x', '--maximum-exploration', default=100000, type=int, help='Maximum exploration step')\n",
    "    parser.add_argument('-fsm', '--first-step-memory', default=0, type=float,\n",
    "                        help='Number of initial steps for just filling the memory')\n",
    "    parser.add_argument('-rs', '--replay-steps', default=10, type=float, help='Steps between updating the network')\n",
    "    parser.add_argument('-nn', '--number-nodes', default=256, type=int, help='Number of nodes in each layer of NN')\n",
    "    parser.add_argument('-tt', '--target-type', choices=['DQN', 'DDQN'], default='DDQN')\n",
    "    parser.add_argument('-mt', '--memory', choices=['UER', 'PER'], default='UER')\n",
    "    parser.add_argument('-pl', '--prioritization-scale', default=0.5, type=float, help='Scale for prioritization')\n",
    "\n",
    "    parser.add_argument('-gn', '--gpu-num', default='2', type=str, help='Number of GPU to use')\n",
    "    parser.add_argument('-test', '--test', action='store_true', help='Enable the test phase if \"store_false\"')\n",
    "\n",
    "    # Game Parameters\n",
    "    parser.add_argument('-k', '--agents-number', default=N_AGENT, type=int, help='The number of agents')\n",
    "    parser.add_argument('-g', '--grid-size', default=15, type=int, help='Grid size')\n",
    "    parser.add_argument('-ts', '--max-timestep', default=100, type=int, help='Maximum number of timesteps per episode')\n",
    "\n",
    "    # parser.add_argument('-rw', '--reward-mode', choices=[0, 1, 2], type=int, default=1, help='Mode of the reward,'\n",
    "    #                                                                                          '0: Only terminal rewards'\n",
    "    #                                                                                          '1: Partial rewards '\n",
    "    #                                                                                          '(number of unoccupied landmarks'\n",
    "    #                                                                                          '2: Full rewards '\n",
    "    #                                                                                          '(sum of dinstances of agents to landmarks)')\n",
    "\n",
    "    parser.add_argument('-rm', '--max-random-moves', default=0, type=int,\n",
    "                        help='Maximum number of random initial moves for the agents')\n",
    "\n",
    "\n",
    "    # Visualization Parameters\n",
    "    parser.add_argument('-r', '--render', action='store_true', help='Turn on visualization if \"store_false\"')\n",
    "    # parser.add_argument('-r', '--render', action='store_false', help='Turn on visualization if \"store_false\"')\n",
    "    parser.add_argument('-re', '--recorder', action='store_false', help='Store the visualization as a movie '\n",
    "                                                                       'if \"store_false\"')\n",
    "\n",
    "    args = vars(parser.parse_args())\n",
    "    # os.environ['CUDA_VISIBLE_DEVICES'] = args['gpu_num']\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "\n",
    "    env = Environment(args)\n",
    "\n",
    "    state_size = env.env.state_size\n",
    "    action_space = env.env.action_space\n",
    "\n",
    "    all_agents = []\n",
    "    # TODO: Remember to change this.\n",
    "    # brain_file = get_name_brain(args, 0)\n",
    "    # brain = Brain(state_size, action_space, brain_file, args)\n",
    "    for b_idx in range(args['agents_number']):\n",
    "        brain_file = get_name_brain(args, b_idx)\n",
    "        all_agents.append(Agent(state_size, action_space, b_idx, brain_file, args))\n",
    "        # all_agents.append(Agent(state_size, action_space, b_idx, brain_file, args,brain))\n",
    "\n",
    "    rewards_file = get_name_rewards(args)\n",
    "    timesteps_file = get_name_timesteps(args)\n",
    "\n",
    "    env.run(all_agents, rewards_file, timesteps_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "communist-fellowship",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sistem belirtilen yolu bulam�yor.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alive-black",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
